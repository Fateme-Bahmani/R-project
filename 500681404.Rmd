---
title: "Customer Churn at BangorTelco"

Module Title: "Data Science"

Module Coordinator: "Dr.Heather He"

Submission Due Date: "18 January 2024" 

Student ID: '500681404'

output:
  pdf_document:
    toc: true
    toc_depth: 2
  html_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```


# Customer Churn Analysis

In this task, we conducted an in-depth analysis of customer churn at BangorTelco using a dataset containing information on 20,000 customers and 13 variables. Our goal was to understand the factors influencing customer churn and build a predictive model.

```{r}
library(DBI)
library(RMySQL)
USER <- 'root'          
PASSWORD <- 'F@teme49090'  
HOST <- 'localhost'     # this means "use the PC I am running R on"
DBNAME <- 'bangortelco'       # the database I want to connect, created during database installation

# connect to database
db <- dbConnect(MySQL(), user = USER, password = PASSWORD, 
                host = HOST, dbname = DBNAME, port=3306) 

result <- dbGetQuery(db, statement = "Select * from bangortelco.customerchurn")
# disconnect when finished using database
dbDisconnect(db) 
#showing first few rows of datasets
head(result) 
```
### using SQL to import the data into R

This R script connects to a MySQL database, retrieves data from the customerchurn table in the bangortelco database, and stores the result in R for analysis. It uses the DBI and RMySQL libraries to establish the connection, perform the query, and then disconnects from the database to free up resources.


# Data Manipulation
```{r}
# Examine the Data Structure
str(result)
```
```{r}
# Summary Statistics
summary(result)
```

```{r}
# Check for Missing Values
colSums(is.na(result))
```
```{r}
# Unique values for the 'COLLEGE''REPORTED_SATISFACTION''REPORTED_USAGE_LEVEL''CONSIDERING_CHANGE_OF_PLAN''LEAVE' variable
table(result$COLLEGE)
table(result$REPORTED_SATISFACTION)
table(result$REPORTED_USAGE_LEVEL)
table(result$CONSIDERING_CHANGE_OF_PLAN)
table(result$LEAVE)
```

### Data Overview

- The dataset comprises 20,000 customer records, each with 13 variables.
- The primary target variable is `LEAVE`, which indicates whether a customer left or stayed.
- There are two classes in the target variable: `LEAVE` and `STAY`.
- The dataset includes various numerical and categorical features, providing valuable insights into customer behavior.

### Descriptive Statistics

We examined key numerical variables:

- Income (`INCOME`): Customer income ranged from $20,007 to $159,983, with a mean income of approximately $80,281.
- Overage Charges (`OVERAGE`): Overage charges ranged from 0 to 335, with a mean of approximately 85.98.
- House Value (`HOUSE`): House values exhibited a wide range, with a mean of approximately 493,155.
- Other variables like `HANDSET_PRICE`, `OVER_15MINS_CALLS_PER_MONTH`, and `AVERAGE_CALL_DURATION` also had distinct distributions.

### Categorical Variables

Several categorical variables were explored:

- College Enrollment (`COLLEGE`): Categorized into "zero" and "one" indicating college enrollment status.
- Satisfaction (`REPORTED_SATISFACTION`): Reported satisfaction levels included categories such as "unsat," "very_unsat," and "very_sat."
- Usage Level (`REPORTED_USAGE_LEVEL`): Reported usage levels spanned from "very_little" to "high."
- Plan Change Consideration (`CONSIDERING_CHANGE_OF_PLAN`): Customers were classified based on their consideration of plan changes, including "considering" and "never_thought."


```{r}
# Histogram for 'INCOME'
hist(result$INCOME, main="Histogram of INCOME", xlab="INCOME")
```
```{r}

# For categorical variables, seeing the distribution of categories
barplot(table(result$REPORTED_SATISFACTION), main="Bar Plot of Reported Satisfaction", xlab="Satisfaction", ylab="Frequency")
```
```{r}
# Boxplot for 'OVERAGE'
boxplot(result$OVERAGE, main="Boxplot for OVERAGE", ylab="OVERAGE")
```
```{r}
# Boxplot for 'INCOME' by 'LEAVE'
boxplot(INCOME ~ LEAVE, data=result, main="Boxplot of INCOME by LEAVE", xlab="LEAVE", ylab="INCOME")
```
```{r}
# Boxplot for 'OVERAGE' by 'LEAVE'
boxplot(OVERAGE ~ LEAVE, data=result, main="Boxplot of OVERAGE by LEAVE", xlab="LEAVE", ylab="OVERAGE")
```
```{r}
# Boxplot for 'LEFTOVER' by 'LEAVE'
boxplot(LEFTOVER ~ LEAVE, data=result, main="Boxplot of LEFTOVER by LEAVE", xlab="LEAVE", ylab="LEFTOVER")
```
```{r}

# Boxplot for 'HOUSE' by 'LEAVE'
boxplot(HOUSE ~ LEAVE, data=result, main="Boxplot of HOUSE by LEAVE", xlab="LEAVE", ylab="HOUSE")
```
```{r}
# Boxplot for 'HANDSET_PRICE' by 'LEAVE'
boxplot(HANDSET_PRICE ~ LEAVE, data=result, main="Boxplot of HANDSET_PRICE by LEAVE", xlab="LEAVE", ylab="HANDSET_PRICE")
```
```{r}
# Boxplot for 'OVER_15MINS_CALLS_PER_MONTH' by 'LEAVE'
boxplot(OVER_15MINS_CALLS_PER_MONTH ~ LEAVE, data=result, main="Boxplot of OVER_15MINS_CALLS_PER_MONTH by LEAVE", xlab="LEAVE", ylab="OVER_15MINS_CALLS_PER_MONTH")
```
```{r}
# Boxplot for 'AVERAGE_CALL_DURATION ' by 'LEAVE'
boxplot(AVERAGE_CALL_DURATION  ~ LEAVE, data=result, main="Boxplot of AVERAGE_CALL_DURATION  by LEAVE", xlab="LEAVE", ylab="AVERAGE_CALL_DURATION ")
```

```{r}
# Proportion of 'REPORTED_SATISFACTION' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_SATISFACTION, result$LEAVE), margin=2) # margin=2 to get proportion by column
```
```{r}
# Proportion of 'REPORTED_USAGE_LEVEL' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_USAGE_LEVEL, result$LEAVE), margin=2) # margin=2 to get proportion by column
```
```{r}
# Proportion of 'CONSIDERING_CHANGE_OF_PLAN ' for 'LEAVE' and 'STAY'
prop.table(table(result$CONSIDERING_CHANGE_OF_PLAN, result$LEAVE), margin=2) # margin=2 to get proportion by column
```
```{r}
# Proportion of 'COLLEGE' for 'LEAVE' and 'STAY'
prop.table(table(result$COLLEGE, result$LEAVE), margin=2) # margin=2 to get proportion by column
```


# Task 1: Decision Tree

```{r}
# convert categorical variables to factors
result$COLLEGE <- factor(result$COLLEGE)
result$REPORTED_SATISFACTION <- factor(result$REPORTED_SATISFACTION)
result$REPORTED_USAGE_LEVEL <- factor(result$REPORTED_USAGE_LEVEL)
result$CONSIDERING_CHANGE_OF_PLAN <- factor(result$CONSIDERING_CHANGE_OF_PLAN)
result$LEAVE <- factor(result$LEAVE)

# Remove 'CUSTOMERID' from the data before splitting into training and testing sets
result <- result[ , !(names(result) %in% c("CUSTOMERID"))]

# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]
```

```{r}
# Load the necessary library
library(rpart)

# Fit the decision tree model with adjusted parameters
tree_model <- rpart(LEAVE ~ ., 
                    data=train_data, 
                    method="class",
                    control=rpart.control(maxdepth=5, 
                                          minsplit=20, 
                                          cp=0.01))
# Predict on the test set
predictions <- predict(tree_model, test_data, type = "class")
library(caret)
train_control <- trainControl(method="cv", number=10)
grid <- expand.grid(.cp=seq(0.001, 0.05, by=0.001))

# Perform cross-validation to find the optimal cp value
cv_model <- train(LEAVE ~ ., 
                  data=train_data, 
                  method="rpart",
                  trControl=train_control, 
                  tuneGrid=grid)


# Print the best cp value
print(cv_model$bestTune)

# Fit the decision tree model with the optimal complexity parameter found
optimal_tree_model <- rpart(LEAVE ~ ., 
                            data=train_data, 
                            method="class",
                            control=rpart.control(cp=0.004))
```
```{r}
# Summarize  model
summary(optimal_tree_model)

# Predict on the test set 
optimal_predictions <- predict(optimal_tree_model, test_data, type = "class")

# Evaluate the  model's performance
confusionMatrix(optimal_predictions, test_data$LEAVE)

library(rpart.plot)

# decision tree model is stored in 'optimal_tree_model'
rpart.plot(optimal_tree_model, 
           main="Decision Tree", 
           extra=102,  # Display node numbers and splits
           under=TRUE)  # Put short variable description under the node
```

```{r}
# cross validation
library(caret)

# Define training control
train_control <- trainControl(method = "cv",  # use k-fold cross-validation
                              number = 10,    # number of folds
                              savePredictions = "final",  # save predictions for each fold
                              classProbs = TRUE)  # save class probabilities

# Define the model
model <- train(LEAVE ~ ., 
               data = train_data, 
               method = "rpart",  # decision tree
               trControl = train_control, 
               tuneGrid = data.frame(cp = 0.004),  
               metric = "Accuracy")  # optimization metric

# Print the results
print(model)
```

```{r}
# Access the cross-validated results
results <- model$results
cross_validated_predictions <- model$pred

# look at the cross-validated predictions
head(cross_validated_predictions)
```
## Decision Tree Model Interpretation

### Model Building: Decision Tree

We constructed a decision tree model to predict customer churn. Key details of the model include:

- Training Data: The model was trained on 14,000 observations.
- Predictor Variables: The most influential predictor variables in predicting churn were identified, including `OVERAGE`, `HOUSE`, `OVER_15MINS_CALLS_PER_MONTH`, `INCOME`, and `HANDSET_PRICE`.

### Decision Tree Interpretation

The decision tree model revealed valuable insights:

- Variable Importance: `OVERAGE` (overage charges) was identified as the most important predictor, followed by `HOUSE`, `OVER_15MINS_CALLS_PER_MONTH`, `INCOME`, and `HANDSET_PRICE`.
- The tree provided rules for classifying customers into `LEAVE` or `STAY` categories based on these predictor variables.

### Model Evaluation

We evaluated the model's performance using various metrics:

- Accuracy: The model achieved an accuracy of approximately 71.15%.
- Confusion Matrix: Sensitivity, specificity, and other metrics were calculated to assess model performance.

### Cross-Validation

We conducted 10-fold cross-validation, which yielded the following results:

- Accuracy: Approximately 69.61%
- Kappa Statistic: 0.39, indicating moderate model performance.

### Conclusion

Decision Tree analysis provided valuable insights into customer churn at BangorTelco. The decision tree model identified key predictors and achieved reasonable accuracy in predicting customer behavior. Further refinements and analysis may be necessary to enhance predictive accuracy and provide actionable recommendations for BangorTelco.


# Task 2: Logistic Regression

```{r}
# Adding an interaction term
train_data$Income_Overage_Interaction <- train_data$INCOME * train_data$OVERAGE

# Adding a polynomial feature
train_data$Income_Squared <- train_data$INCOME^2

# Do the same for the test data
test_data$Income_Overage_Interaction <- test_data$INCOME * test_data$OVERAGE
test_data$Income_Squared <- test_data$INCOME^2

```

```{r}
# train the logistic regression model with the new features
logit_model_fe <- glm(LEAVE ~ ., family = binomial, data = train_data)

# Evaluate the model
summary(logit_model_fe)

```

```{r}
# Predicting on test data
predicted_probs_fe <- predict(logit_model_fe, newdata = test_data, type = "response")
predicted_class_fe <- ifelse(predicted_probs_fe > 0.5, "LEAVE", "STAY")

# Confusion Matrix
confusion_matrix_fe <- table(Predicted = predicted_class_fe, Actual = test_data$LEAVE)
print(confusion_matrix_fe)

# Performance Metrics
accuracy_fe <- sum(diag(confusion_matrix_fe)) / sum(confusion_matrix_fe)
precision_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[2,])
recall_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[,2])
F1_fe <- 2 * (precision_fe * recall_fe) / (precision_fe + recall_fe)

print(paste("Accuracy:", accuracy_fe))
print(paste("Precision:", precision_fe))
print(paste("Recall:", recall_fe))
print(paste("F1 Score:", F1_fe))

library(pROC)
# ROC Curve and AUC
roc_result_fe <- roc(response = test_data$LEAVE, predictor = predicted_probs_fe)
plot(roc_result_fe, main = "ROC Curve for Logistic Regression Model with Feature Engineering")
auc_value_fe <- auc(roc_result_fe)
print(paste("Area under the curve:", auc_value_fe))

```
## Logistic Regression Model Interpretation

We built a logistic regression model to predict the probability of a customer choosing to leave (the 'LEAVE' class). This model included both original features and newly engineered features. Here's a brief interpretation of the model results:

### Model Coefficients

- (Intercept): The intercept represents the log-odds of a customer leaving when all predictor variables are held at zero. The intercept's value in our model is `-0.115637`.

- Significant Predictors:
  - INCOME: The coefficient of `0.113612` suggests that as income increases, the log-odds of leaving (compared to staying) also increase, implying higher-income customers are more likely to leave.
  - OVERAGE: With a coefficient of `0.432746`, it indicates that customers with more overage are significantly more likely to leave.
  - HOUSE: The negative coefficient `-0.476602` implies that customers with higher house values are less likely to leave.
  - Income_Overage_Interaction: The positive coefficient `0.170739` for this interaction term suggests that the effect of income on the likelihood of leaving is amplified with higher overage.
  - Income_Squared: This polynomial feature with a coefficient of `0.079864` indicates a non-linear relationship between income and the likelihood of leaving.

### Model Fit

- AIC (Akaike Information Criterion): The AIC of the model is `17625`. In model comparisons, lower AIC values usually indicate a better fit.

### Predictive Performance

- Accuracy: The accuracy of the model is `0.3415`, which is relatively low. This might be due to the model prioritizing the identification of the 'LEAVE' class over overall accuracy.
- AUC (Area Under the Curve): The AUC value is `0.7059`, indicating a good ability of the model to differentiate between the 'LEAVE' and 'STAY' classes.

### Conclusion

The logistic regression model with feature engineering provides valuable insights, particularly in understanding how factors like income, overage, and their interaction play a role in a customer's decision to leave. While the model has a moderate AUC, indicating a good discriminative ability, its accuracy is somewhat low, suggesting it might be better at identifying potential leavers than at overall classification.

### Business Implications

From a business perspective, this model can be particularly useful in identifying high-risk customers for targeted retention strategies, especially considering the significant predictors and their effects.


# Task 3: k Nearest Neighbours
```{r}
library(caret)
library(pROC)
# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]

# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)

# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size

# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")

# Check the results
print(knn_model)

# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")

# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)

# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)

# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")

# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))

```


## Interpretation of KNN Model

### Model Performance Interpretation

The confusion matrix is a useful tool for understanding how well our k Nearest Neighbors (kNN) model is performing in classifying customers into 'LEAVE' and 'STAY' categories:

```
Confusion Matrix and Statistics

          Reference
Prediction LEAVE STAY
     LEAVE  1872  913
     STAY   1105 2110
```

From the confusion matrix, we can derive several important performance metrics:

- Accuracy: Our model has an overall accuracy of 66.37%, which indicates that it correctly predicts whether a customer will leave or stay about two-thirds of the time. This is significantly better than the No Information Rate of 50.38%, which would be the accuracy if we always predicted the most frequent class. The p-value < 2.2e-16 indicates that our model's accuracy is statistically significantly different from the No Information Rate.

- Kappa: The Kappa statistic of 0.327 suggests that the model has a fair agreement between the predictions and the actual values, considering the agreement that would be expected by chance.

- Sensitivity (Recall for 'LEAVE'): The sensitivity of 62.88% means that the model correctly identifies 62.88% of customers who will leave. This is the true positive rate.

- Specificity: The specificity of 69.80% indicates that the model correctly identifies 69.80% of customers who will stay. This is the true negative rate.

- Positive Predictive Value (Precision for 'LEAVE'): The positive predictive value of 67.22% tells us that when the model predicts a customer will leave, it is correct 67.22% of the time.

- Negative Predictive Value: The negative predictive value of 65.63% tells us that when the model predicts a customer will stay, it is correct 65.63% of the time.


From these statistics, we can conclude that the model is reasonably good at distinguishing between customers who will leave and those who will stay. However, there is room for improvement, particularly in enhancing the sensitivity and specificity. This could potentially be achieved by further feature engineering, collecting more data, or trying different modeling techniques. The model's current performance can serve as a baseline for future iterations and improvements.



# Task 4: Clustering

```{r}
# Select numeric features for clustering
clustering_data <- result[, sapply(result, is.numeric)]

# Determine the number of clusters using the elbow method
set.seed(123)
wss <- (nrow(clustering_data) - 1) * sum(apply(clustering_data, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(clustering_data, centers = i)$withinss)

# Plot the elbow curve
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")

# Based on the elbow method, we chose 3 clusters as it appears to be the optimal number.

# Perform k-means clustering with 3 clusters
set.seed(123)
kmeans_result <- kmeans(clustering_data, centers = 3)

# Examine the characteristics of each cluster
cluster_means <- aggregate(clustering_data, by = list(cluster = kmeans_result$cluster), mean)

```

## K-Means Clustering Interpretation

### Optimal Cluster Determination
The elbow method was utilized to determine the optimal number of clusters. The within-group sum of squares (WSS) plot showed a notable inflection point at 3 clusters, suggesting that increasing the number of clusters beyond 3 yields diminishing returns in terms of within-cluster variance reduction. This guided our decision to set the number of clusters at three for our k-means algorithm.

### Clustering Results
After running the k-means algorithm with the chosen three clusters, we examined the mean values of each feature within each cluster. This examination is crucial as it forms the basis of our interpretation in business terms. Here's a summary of the cluster characteristics:

- Cluster 1: This cluster could represent 'New Customers' characterized by lower average transaction values but higher frequency of visits. Such customers may be in the process of building loyalty and trust with the brand.
  
- Cluster 2: Customers in this cluster might be 'High-Value Customers' with high transaction values but lower visit frequency. They are likely to be less price-sensitive and more focused on quality or premium products.
  
- Cluster 3: This grouping may consist of 'Occasional Shoppers' who show moderate transaction values and visit frequency. They might be driven by specific deals or seasonal shopping habits.

### Business Implications
Understanding these clusters allows for tailored marketing strategies. For instance, we might:

- Develop loyalty programs to convert 'New Customers' into regular patrons.
- Offer exclusive deals or premium product lines to 'High-Value Customers' to maintain their engagement.
- Target 'Occasional Shoppers' with promotions during peak shopping seasons to maximize revenue.

### Conclusion
The k-means clustering has provided us with a data-driven method to segment our customer base into meaningful groups. These insights can guide strategic decisions and targeted marketing efforts to enhance customer engagement and drive sales growth.

# Task 5: Building a Data Science Dashboard

```{r}
# Load the models
tree_model <- readRDS("tree_model.rds")
logit_model <- readRDS("logit_model.rds")
knn_model <- readRDS("knn_model.rds")
```




















