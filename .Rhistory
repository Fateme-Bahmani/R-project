boxplot(result$OVERAGE, main="Boxplot for OVERAGE", ylab="OVERAGE")
# Boxplot for 'INCOME' by 'LEAVE'
boxplot(INCOME ~ LEAVE, data=result, main="Boxplot of INCOME by LEAVE", xlab="LEAVE", ylab="INCOME")
# Boxplot for 'OVERAGE' by 'LEAVE'
boxplot(OVERAGE ~ LEAVE, data=result, main="Boxplot of OVERAGE by LEAVE", xlab="LEAVE", ylab="OVERAGE")
# Boxplot for 'LEFTOVER' by 'LEAVE'
boxplot(LEFTOVER ~ LEAVE, data=result, main="Boxplot of LEFTOVER by LEAVE", xlab="LEAVE", ylab="LEFTOVER")
# Boxplot for 'HOUSE' by 'LEAVE'
boxplot(HOUSE ~ LEAVE, data=result, main="Boxplot of HOUSE by LEAVE", xlab="LEAVE", ylab="HOUSE")
# Boxplot for 'HANDSET_PRICE' by 'LEAVE'
boxplot(HANDSET_PRICE ~ LEAVE, data=result, main="Boxplot of HANDSET_PRICE by LEAVE", xlab="LEAVE", ylab="HANDSET_PRICE")
# Boxplot for 'OVER_15MINS_CALLS_PER_MONTH' by 'LEAVE'
boxplot(OVER_15MINS_CALLS_PER_MONTH ~ LEAVE, data=result, main="Boxplot of OVER_15MINS_CALLS_PER_MONTH by LEAVE", xlab="LEAVE", ylab="OVER_15MINS_CALLS_PER_MONTH")
# Boxplot for 'AVERAGE_CALL_DURATION ' by 'LEAVE'
boxplot(AVERAGE_CALL_DURATION  ~ LEAVE, data=result, main="Boxplot of AVERAGE_CALL_DURATION  by LEAVE", xlab="LEAVE", ylab="AVERAGE_CALL_DURATION ")
# Proportion of 'REPORTED_SATISFACTION' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_SATISFACTION, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'REPORTED_USAGE_LEVEL' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_USAGE_LEVEL, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'CONSIDERING_CHANGE_OF_PLAN ' for 'LEAVE' and 'STAY'
prop.table(table(result$CONSIDERING_CHANGE_OF_PLAN, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'COLLEGE' for 'LEAVE' and 'STAY'
prop.table(table(result$COLLEGE, result$LEAVE), margin=2) # margin=2 to get proportion by column
# convert categorical variables to factors
result$COLLEGE <- factor(result$COLLEGE)
result$REPORTED_SATISFACTION <- factor(result$REPORTED_SATISFACTION)
result$REPORTED_USAGE_LEVEL <- factor(result$REPORTED_USAGE_LEVEL)
result$CONSIDERING_CHANGE_OF_PLAN <- factor(result$CONSIDERING_CHANGE_OF_PLAN)
result$LEAVE <- factor(result$LEAVE)
# Remove 'CUSTOMERID' from the data before splitting into training and testing sets
result <- result[ , !(names(result) %in% c("CUSTOMERID"))]
# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]
# Load the necessary library
library(rpart)
# Fit the decision tree model with adjusted parameters
tree_model <- rpart(LEAVE ~ .,
data=train_data,
method="class",
control=rpart.control(maxdepth=5,
minsplit=20,
cp=0.01))
# Predict on the test set
predictions <- predict(tree_model, test_data, type = "class")
library(caret)
train_control <- trainControl(method="cv", number=10)
grid <- expand.grid(.cp=seq(0.001, 0.05, by=0.001))
# Perform cross-validation to find the optimal cp value
cv_model <- train(LEAVE ~ .,
data=train_data,
method="rpart",
trControl=train_control,
tuneGrid=grid)
# Print the best cp value
print(cv_model$bestTune)
# Fit the decision tree model with the optimal complexity parameter found
optimal_tree_model <- rpart(LEAVE ~ .,
data=train_data,
method="class",
control=rpart.control(cp=0.004))
# Summarize the new model
summary(optimal_tree_model)
# Predict on the test set with the new model
optimal_predictions <- predict(optimal_tree_model, test_data, type = "class")
# Evaluate the new model's performance
confusionMatrix(optimal_predictions, test_data$LEAVE)
library(rpart.plot)
# decision tree model is stored in 'optimal_tree_model'
rpart.plot(optimal_tree_model,
main="Decision Tree",
extra=102,  # Display node numbers and splits
under=TRUE)  # Put short variable description under the node
# cross validation
library(caret)
# Define training control
train_control <- trainControl(method = "cv",  # use k-fold cross-validation
number = 10,    # number of folds
savePredictions = "final",  # save predictions for each fold
classProbs = TRUE)  # save class probabilities
# Define the model
model <- train(LEAVE ~ .,
data = train_data,
method = "rpart",  # decision tree
trControl = train_control,
tuneGrid = data.frame(cp = 0.004),  # use the previously determined cp value
metric = "Accuracy")  # optimization metric
# Print the results
print(model)
# Access the cross-validated results
results <- model$results
cross_validated_predictions <- model$pred
# look at the cross-validated predictions
head(cross_validated_predictions)
# Adding an interaction term
train_data$Income_Overage_Interaction <- train_data$INCOME * train_data$OVERAGE
# Adding a polynomial feature
train_data$Income_Squared <- train_data$INCOME^2
# Do the same for the test data
test_data$Income_Overage_Interaction <- test_data$INCOME * test_data$OVERAGE
test_data$Income_Squared <- test_data$INCOME^2
# train the logistic regression model with the new features
logit_model_fe <- glm(LEAVE ~ ., family = binomial, data = train_data)
# Evaluate the model
summary(logit_model_fe)
# Predicting on test data
predicted_probs_fe <- predict(logit_model_fe, newdata = test_data, type = "response")
predicted_class_fe <- ifelse(predicted_probs_fe > 0.5, "LEAVE", "STAY")
# Confusion Matrix
confusion_matrix_fe <- table(Predicted = predicted_class_fe, Actual = test_data$LEAVE)
print(confusion_matrix_fe)
# Performance Metrics
accuracy_fe <- sum(diag(confusion_matrix_fe)) / sum(confusion_matrix_fe)
precision_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[2,])
recall_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[,2])
F1_fe <- 2 * (precision_fe * recall_fe) / (precision_fe + recall_fe)
print(paste("Accuracy:", accuracy_fe))
print(paste("Precision:", precision_fe))
print(paste("Recall:", recall_fe))
print(paste("F1 Score:", F1_fe))
library(pROC)
# ROC Curve and AUC
roc_result_fe <- roc(response = test_data$LEAVE, predictor = predicted_probs_fe)
plot(roc_result_fe, main = "ROC Curve for Logistic Regression Model with Feature Engineering")
auc_value_fe <- auc(roc_result_fe)
print(paste("Area under the curve:", auc_value_fe))
# Load the necessary libraries
library(caret)
library(pROC)
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))
print(confusion_matrix)
library(caret)
library(pROC)
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))
knitr::opts_chunk$set(echo = TRUE)
# Select numeric features for clustering
clustering_data <- result[, sapply(result, is.numeric)]
library(DBI)
library(RMySQL)
USER <- 'root'
PASSWORD <- 'F@teme49090'
HOST <- 'localhost'     # this means "use the PC I am running R on"
DBNAME <- 'bangortelco'       # the database I want to connect, created during database installation
# connect to database
db <- dbConnect(MySQL(), user = USER, password = PASSWORD,
host = HOST, dbname = DBNAME, port=3306)
result <- dbGetQuery(db, statement = "Select * from bangortelco.customerchurn")
# disconnect when finished using database
dbDisconnect(db)
#showing first few rows of datasets
head(result)
library(DBI)
library(RMySQL)
USER <- 'root'
PASSWORD <- 'F@teme49090'
HOST <- 'localhost'     # this means "use the PC I am running R on"
DBNAME <- 'bangortelco'       # the database I want to connect, created during database installation
# connect to database
db <- dbConnect(MySQL(), user = USER, password = PASSWORD,
host = HOST, dbname = DBNAME, port=3306)
result <- dbGetQuery(db, statement = "Select * from bangortelco.customerchurn")
# disconnect when finished using database
dbDisconnect(db)
#showing first few rows of datasets
head(result)
# Examine the Data Structure
str(result)
# Summary Statistics
summary(result)
# Check for Missing Values
colSums(is.na(result))
# Unique values for the 'COLLEGE''REPORTED_SATISFACTION''REPORTED_USAGE_LEVEL''CONSIDERING_CHANGE_OF_PLAN''LEAVE' variable
table(result$COLLEGE)
table(result$REPORTED_SATISFACTION)
table(result$REPORTED_USAGE_LEVEL)
table(result$CONSIDERING_CHANGE_OF_PLAN)
table(result$LEAVE)
# Histogram for 'INCOME'
hist(result$INCOME, main="Histogram of INCOME", xlab="INCOME")
# For categorical variables, seeing the distribution of categories
barplot(table(result$REPORTED_SATISFACTION), main="Bar Plot of Reported Satisfaction", xlab="Satisfaction", ylab="Frequency")
# Boxplot for 'OVERAGE'
boxplot(result$OVERAGE, main="Boxplot for OVERAGE", ylab="OVERAGE")
# Boxplot for 'INCOME' by 'LEAVE'
boxplot(INCOME ~ LEAVE, data=result, main="Boxplot of INCOME by LEAVE", xlab="LEAVE", ylab="INCOME")
# Boxplot for 'OVERAGE' by 'LEAVE'
boxplot(OVERAGE ~ LEAVE, data=result, main="Boxplot of OVERAGE by LEAVE", xlab="LEAVE", ylab="OVERAGE")
# Boxplot for 'LEFTOVER' by 'LEAVE'
boxplot(LEFTOVER ~ LEAVE, data=result, main="Boxplot of LEFTOVER by LEAVE", xlab="LEAVE", ylab="LEFTOVER")
# Boxplot for 'HOUSE' by 'LEAVE'
boxplot(HOUSE ~ LEAVE, data=result, main="Boxplot of HOUSE by LEAVE", xlab="LEAVE", ylab="HOUSE")
# Boxplot for 'HANDSET_PRICE' by 'LEAVE'
boxplot(HANDSET_PRICE ~ LEAVE, data=result, main="Boxplot of HANDSET_PRICE by LEAVE", xlab="LEAVE", ylab="HANDSET_PRICE")
# Boxplot for 'OVER_15MINS_CALLS_PER_MONTH' by 'LEAVE'
boxplot(OVER_15MINS_CALLS_PER_MONTH ~ LEAVE, data=result, main="Boxplot of OVER_15MINS_CALLS_PER_MONTH by LEAVE", xlab="LEAVE", ylab="OVER_15MINS_CALLS_PER_MONTH")
# Boxplot for 'AVERAGE_CALL_DURATION ' by 'LEAVE'
boxplot(AVERAGE_CALL_DURATION  ~ LEAVE, data=result, main="Boxplot of AVERAGE_CALL_DURATION  by LEAVE", xlab="LEAVE", ylab="AVERAGE_CALL_DURATION ")
# Proportion of 'REPORTED_SATISFACTION' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_SATISFACTION, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'REPORTED_USAGE_LEVEL' for 'LEAVE' and 'STAY'
prop.table(table(result$REPORTED_USAGE_LEVEL, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'CONSIDERING_CHANGE_OF_PLAN ' for 'LEAVE' and 'STAY'
prop.table(table(result$CONSIDERING_CHANGE_OF_PLAN, result$LEAVE), margin=2) # margin=2 to get proportion by column
# Proportion of 'COLLEGE' for 'LEAVE' and 'STAY'
prop.table(table(result$COLLEGE, result$LEAVE), margin=2) # margin=2 to get proportion by column
# convert categorical variables to factors
result$COLLEGE <- factor(result$COLLEGE)
result$REPORTED_SATISFACTION <- factor(result$REPORTED_SATISFACTION)
result$REPORTED_USAGE_LEVEL <- factor(result$REPORTED_USAGE_LEVEL)
result$CONSIDERING_CHANGE_OF_PLAN <- factor(result$CONSIDERING_CHANGE_OF_PLAN)
result$LEAVE <- factor(result$LEAVE)
# Remove 'CUSTOMERID' from the data before splitting into training and testing sets
result <- result[ , !(names(result) %in% c("CUSTOMERID"))]
# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]
# Load the necessary library
library(rpart)
# Fit the decision tree model with adjusted parameters
tree_model <- rpart(LEAVE ~ .,
data=train_data,
method="class",
control=rpart.control(maxdepth=5,
minsplit=20,
cp=0.01))
# Predict on the test set
predictions <- predict(tree_model, test_data, type = "class")
library(caret)
train_control <- trainControl(method="cv", number=10)
grid <- expand.grid(.cp=seq(0.001, 0.05, by=0.001))
# Perform cross-validation to find the optimal cp value
cv_model <- train(LEAVE ~ .,
data=train_data,
method="rpart",
trControl=train_control,
tuneGrid=grid)
# Print the best cp value
print(cv_model$bestTune)
# Fit the decision tree model with the optimal complexity parameter found
optimal_tree_model <- rpart(LEAVE ~ .,
data=train_data,
method="class",
control=rpart.control(cp=0.004))
# Summarize the new model
summary(optimal_tree_model)
# Predict on the test set with the new model
optimal_predictions <- predict(optimal_tree_model, test_data, type = "class")
# Evaluate the new model's performance
confusionMatrix(optimal_predictions, test_data$LEAVE)
library(rpart.plot)
# decision tree model is stored in 'optimal_tree_model'
rpart.plot(optimal_tree_model,
main="Decision Tree",
extra=102,  # Display node numbers and splits
under=TRUE)  # Put short variable description under the node
# cross validation
library(caret)
# Define training control
train_control <- trainControl(method = "cv",  # use k-fold cross-validation
number = 10,    # number of folds
savePredictions = "final",  # save predictions for each fold
classProbs = TRUE)  # save class probabilities
# Define the model
model <- train(LEAVE ~ .,
data = train_data,
method = "rpart",  # decision tree
trControl = train_control,
tuneGrid = data.frame(cp = 0.004),  # use the previously determined cp value
metric = "Accuracy")  # optimization metric
# Print the results
print(model)
# Access the cross-validated results
results <- model$results
cross_validated_predictions <- model$pred
# look at the cross-validated predictions
head(cross_validated_predictions)
# Adding an interaction term
train_data$Income_Overage_Interaction <- train_data$INCOME * train_data$OVERAGE
# Adding a polynomial feature
train_data$Income_Squared <- train_data$INCOME^2
# Do the same for the test data
test_data$Income_Overage_Interaction <- test_data$INCOME * test_data$OVERAGE
test_data$Income_Squared <- test_data$INCOME^2
# train the logistic regression model with the new features
logit_model_fe <- glm(LEAVE ~ ., family = binomial, data = train_data)
# Evaluate the model
summary(logit_model_fe)
# Predicting on test data
predicted_probs_fe <- predict(logit_model_fe, newdata = test_data, type = "response")
predicted_class_fe <- ifelse(predicted_probs_fe > 0.5, "LEAVE", "STAY")
# Confusion Matrix
confusion_matrix_fe <- table(Predicted = predicted_class_fe, Actual = test_data$LEAVE)
print(confusion_matrix_fe)
# Performance Metrics
accuracy_fe <- sum(diag(confusion_matrix_fe)) / sum(confusion_matrix_fe)
precision_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[2,])
recall_fe <- confusion_matrix_fe[2,2] / sum(confusion_matrix_fe[,2])
F1_fe <- 2 * (precision_fe * recall_fe) / (precision_fe + recall_fe)
print(paste("Accuracy:", accuracy_fe))
print(paste("Precision:", precision_fe))
print(paste("Recall:", recall_fe))
print(paste("F1 Score:", F1_fe))
library(pROC)
# ROC Curve and AUC
roc_result_fe <- roc(response = test_data$LEAVE, predictor = predicted_probs_fe)
plot(roc_result_fe, main = "ROC Curve for Logistic Regression Model with Feature Engineering")
auc_value_fe <- auc(roc_result_fe)
print(paste("Area under the curve:", auc_value_fe))
print(confusion_matrix)
# Load the necessary libraries
library(caret)
library(pROC)
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))
# Load the necessary libraries
library(caret)
library(pROC)
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
print(confusion_matrix)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
# Select numeric features for clustering
clustering_data <- result[, sapply(result, is.numeric)]
# Determine the number of clusters using the elbow method
set.seed(123)
wss <- (nrow(clustering_data) - 1) * sum(apply(clustering_data, 2, var))
for (i in 2:15) wss[i] <- sum(kmeans(clustering_data, centers = i)$withinss)
# Plot the elbow curve
plot(1:15, wss, type = "b", xlab = "Number of Clusters", ylab = "Within groups sum of squares")
# Based on the elbow method, we chose 3 clusters as it appears to be the optimal number.
# Perform k-means clustering with 3 clusters
set.seed(123)
kmeans_result <- kmeans(clustering_data, centers = 3)
# Examine the characteristics of each cluster
cluster_means <- aggregate(clustering_data, by = list(cluster = kmeans_result$cluster), mean)
# Load the models
tree_model <- readRDS("tree_model.rds")
logit_model <- readRDS("logit_model.rds")
knn_model <- readRDS("knn_model.rds")
library(caret)
library(pROC)
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))
library(caret)
library(pROC)
# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]
library(caret)
library(pROC)
# Split the data into training and testing sets
set.seed(123) # for reproducible results
training_indices <- sample(1:nrow(result), 0.7 * nrow(result))
train_data <- result[training_indices, ]
test_data <- result[-training_indices, ]
# Normalize the continuous features for both training and testing sets
preproc <- preProcess(train_data[, -ncol(train_data)], method = c("center", "scale"))
train_data_norm <- predict(preproc, train_data)
test_data_norm <- predict(preproc, test_data)
# Set up cross-validation
set.seed(123)
train_control <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)
# Tune the k parameter
grid <- expand.grid(k = 1:20)  # You can adjust the range of k based on your dataset size
# Train the kNN model
knn_model <- train(LEAVE ~ ., data = train_data_norm, method = "knn", tuneGrid = grid, trControl = train_control, metric = "ROC")
# Check the results
print(knn_model)
# Best model's k value
best_k <- knn_model$bestTune$k
cat("The best k value is:", best_k, "\n")
# Make predictions using the best k value
knn_predictions <- predict(knn_model, newdata = test_data_norm)
# Evaluate the model's performance with a confusion matrix
confusion_matrix <- confusionMatrix(knn_predictions, test_data_norm$LEAVE)
print(confusion_matrix)
# For ROC curve and AUC, you need class probabilities
knn_probs <- predict(knn_model, newdata = test_data_norm, type = "prob")
# ROC curve and AUC
roc_response <- roc(response = test_data_norm$LEAVE, predictor = knn_probs[, "LEAVE"])
auc_value <- auc(roc_response)
plot(roc_response, main = paste("ROC Curve for kNN Model (AUC =", round(auc_value, 2), ")"))
